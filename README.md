# Mini_Project: A Large Language Model Analysis from the HuggingFace Leaderboard 2024-2025
# Introduction
(Need more over-all general LLM in the front)
As Large Language Model have been rapidly emerging in the past few years, many techniques like Mixture of Experts, which through combining multiple "sub-models" and activating only when relevant to the prompt/context, aim to scale performance while reducing the computational costs, have been developed.
This project aims to explore the HuggingFace leaderboard dataset, targeting the parameters, the average benchmark scores of MoE models vs non-MoE models, and the CO2 costs of the two model variations. (fix a bit)

# Table of Contents

Data Preparation

Insights/Graphs

Conclusion
